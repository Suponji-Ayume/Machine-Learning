{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 浅层神经网络的简单实现\n",
    "\n",
    "输入层维度为$n_x$，隐藏层维度为$n_h$，输出层维度为$n_y$：\n",
    "$n_x = 4$, $n_h = 8$, $n_y = 1$\n",
    "\n",
    "隐藏层激活函数为$tanh$，输出层激活函数为$sigmoid$。\n",
    "\n",
    "损失函数为交叉熵损失函数：\n",
    "$$  L = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log{\\hat{y}^{(i)}} + (1 - y^{(i)})\\log{(1 - \\hat{y}^{(i)})} $$\n",
    "\n",
    "其中，$m$为样本数，$y^{(i)}$为第$i$个样本的目标值，$\\hat{y}^{(i)}$为第$i$个样本的输出值。\n",
    "\n",
    "前向传播：\n",
    "\n",
    "<img alt=\"前向传播\" height=\"1500\" src=\"./images/前向传播.png\" width=\"2000\"/> \n",
    "\n",
    "\n",
    "反向传播：\n",
    "\n",
    "<img alt=\"反向传播\" height=\"1000\" src=\"./images/反向传播.png\" width=\"2000\"/>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c668eade474ece29"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-13T06:38:57.124464Z",
     "start_time": "2024-01-13T06:38:57.078359Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 数据预处理\n",
    "\n",
    "输入数据的维度为$(4, 80)$，其中$4$为特征数，$80$为样本数。输出数据的维度为$(1, 80)$，其中$80$为样本数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4625629e0453bf60"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 80)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(1, 80)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(4, 20)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(1, 20)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 读取数据\n",
    "origin_data = pd.read_csv('../../Datasets/exercise_data/Iris.csv', header=0, nrows=100, index_col=0)\n",
    "species = origin_data[\"Species\"].unique()\n",
    "origin_data[\"Species\"] = origin_data[\"Species\"].map({species[0]: 0, species[1]: 1})\n",
    "\n",
    "# 打乱数据\n",
    "shuffle_data = origin_data.sample(frac=1)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_X = shuffle_data.iloc[:80, :-1].values.T\n",
    "train_Y = shuffle_data.iloc[:80, -1].values.reshape(1, -1)\n",
    "\n",
    "test_X = shuffle_data.iloc[80:, :-1].values.T\n",
    "test_Y = shuffle_data.iloc[80:, -1].values.reshape(1, -1)\n",
    "\n",
    "display(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T03:57:59.172671Z",
     "start_time": "2024-01-13T03:57:59.147448Z"
    }
   },
   "id": "b63843238236a58a",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 搭建神经网络"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f0f70b936d2d5b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 定义神经网络结构"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d2df3634ffb82c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    定义神经网络结构\n",
    "    :param X: 输入数据，维度为(特征数, 样本数)\n",
    "    :param Y: 输出数据，维度为(1, 样本数)\n",
    "    :return: 输入层维度、隐藏层维度、输出层维度\n",
    "    \"\"\"\n",
    "    # 输入层维度\n",
    "    n_x = X.shape[0]\n",
    "\n",
    "    # 隐藏层维度\n",
    "    n_h = 8\n",
    "\n",
    "    # 输出层维度\n",
    "    n_y = Y.shape[0]\n",
    "    return n_x, n_h, n_y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T04:06:43.652554Z",
     "start_time": "2024-01-13T04:06:43.631273Z"
    }
   },
   "id": "d95dc9cec61b04a3",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 初始化模型参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e79cf1b2f953030"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    初始化模型参数\n",
    "    :param n_x: 输入层维度\n",
    "    :param n_h: 隐藏层维度\n",
    "    :param n_y: 输出层维度\n",
    "    :return: 模型参数\n",
    "    \"\"\"\n",
    "    # 随机种子\n",
    "    np.random.seed(2)\n",
    "\n",
    "    # 随机初始化隐藏层参数\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "\n",
    "    # 随机初始化输出层参数\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    # 使用断言确保数据格式正确\n",
    "    assert W1.shape == (n_h, n_x), \"W1的维度应该是({}, {})\".format(n_h, n_x)\n",
    "    assert b1.shape == (n_h, 1), \"b1的维度应该是({}, 1)\".format(n_h)\n",
    "\n",
    "    assert W2.shape == (n_y, n_h), \"W2的维度应该是({}, {})\".format(n_y, n_h)\n",
    "    assert b2.shape == (n_y, 1), \"b2的维度应该是({}, 1)\".format(n_y)\n",
    "\n",
    "    # 以字典形式返回参数\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T05:40:37.601218Z",
     "start_time": "2024-01-13T05:40:37.572839Z"
    }
   },
   "id": "b55d8460171fcc76",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 sigmoid 函数以及导数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1006ec61049bff5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    sigmoid 函数\n",
    "    :param Z: 输入数据\n",
    "    :return: sigmoid 函数值\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    \"\"\"\n",
    "    sigmoid 函数导数\n",
    "    :param Z: 输入数据\n",
    "    :return: sigmoid 函数导数值\n",
    "    \"\"\"\n",
    "    A = sigmoid(Z)\n",
    "    return A * (1 - A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T05:40:34.980846Z",
     "start_time": "2024-01-13T05:40:34.950151Z"
    }
   },
   "id": "fb2835ec38d6b34b",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 前向传播"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f05b488bd878967"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def forward_propogation(X, parameters):\n",
    "    \"\"\"\n",
    "    前向传播\n",
    "    :param X: 输入数据，维度为(特征数, 样本数)\n",
    "    :param parameters: 模型参数\n",
    "    :return: 隐藏层激活值、输出层激活值\n",
    "    \"\"\"\n",
    "    # 获取模型参数\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # 前向传播计算隐藏层激活值\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "\n",
    "    # 前向传播计算输出层激活值\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    # 使用断言确保数据格式正确\n",
    "    assert A2.shape == (1, X.shape[1]), \"A2的维度应该是(1, {})\".format(X.shape[1])\n",
    "\n",
    "    # 以字典形式返回隐藏层激活值和输出层激活值\n",
    "    cache = {\n",
    "        \"Z1\": Z1,\n",
    "        \"A1\": A1,\n",
    "        \"Z2\": Z2,\n",
    "        \"A2\": A2\n",
    "    }\n",
    "    return cache"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T06:36:14.409999Z",
     "start_time": "2024-01-13T06:36:14.389264Z"
    }
   },
   "id": "2ff11c30fb126d55",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 计算损失函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98521b35569ec718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def loss_function(A2, Y):\n",
    "    \"\"\"\n",
    "    计算损失函数\n",
    "    :param A2: 输出层激活值\n",
    "    :param Y: 输出数据\n",
    "    :return: 损失函数值\n",
    "    \"\"\"\n",
    "    # 求每个样本输出值与目标值的交叉熵\n",
    "    cross_entropy = -np.multiply(np.log(A2), Y) - np.multiply(np.log(1 - A2), (1 - Y))\n",
    "\n",
    "    # 求所有样本交叉熵的均值\n",
    "    loss = np.mean(cross_entropy)\n",
    "\n",
    "    # 将得到的 1x1 损失值矩阵转换为标量\n",
    "    loss = np.squeeze(loss)\n",
    "\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T06:36:12.494041Z",
     "start_time": "2024-01-13T06:36:12.473852Z"
    }
   },
   "id": "5457f7cc9d1d78ad",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 反向传播"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95660c4f552b43b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def back_propogation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    反向传播\n",
    "    :param parameters: 模型参数\n",
    "    :param cache: 前向传播得到的隐藏层激活值和输出层激活值\n",
    "    :param X: 输入数据\n",
    "    :param Y: 输出数据\n",
    "    :return: 模型参数的梯度\n",
    "    \"\"\"\n",
    "    # 获取样本数\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # 获取模型参数\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # 获取前向传播得到的隐藏层激活值和输出层激活值\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    # 计算输出层激活值相对于损失函数的导数\n",
    "    dZ2 = A2 - Y\n",
    "\n",
    "    # 计算输出层参数相对于损失函数的导数\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "    # 计算隐藏层激活值相对于损失函数的导数\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "\n",
    "    # 计算隐藏层参数相对于损失函数的导数\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    # 使用断言确保数据格式正确\n",
    "    assert dW2.shape == W2.shape, \"dW2的维度应该是{}\".format(W2.shape)\n",
    "    assert db2.shape == b2.shape, \"db2的维度应该是{}\".format(b2.shape)\n",
    "    assert dW1.shape == W1.shape, \"dW1的维度应该是{}\".format(W1.shape)\n",
    "    assert db1.shape == b1.shape, \"db1的维度应该是{}\".format(b1.shape)\n",
    "\n",
    "    # 以字典形式返回模型参数的梯度\n",
    "    grads = {\n",
    "        \"dW2\": dW2,\n",
    "        \"db2\": db2,\n",
    "        \"dW1\": dW1,\n",
    "        \"db1\": db1\n",
    "    }\n",
    "\n",
    "    return grads"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T06:52:38.028920Z",
     "start_time": "2024-01-13T06:52:37.979430Z"
    }
   },
   "id": "b4bbfea79c786f78",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.7 更新模型参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8e76bf4160ecb2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=0.005):\n",
    "    \"\"\"\n",
    "    更新模型参数\n",
    "    :param parameters: 模型参数\n",
    "    :param grads: 模型参数的梯度\n",
    "    :param learning_rate: 学习率\n",
    "    :return: 更新后的模型参数\n",
    "    \"\"\"\n",
    "    # 获取模型参数\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # 获取模型参数的梯度\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    # 更新模型参数\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b1 -= learning_rate * db1\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    # 使用断言确保数据格式正确\n",
    "    assert W1.shape == dW1.shape, \"W1的维度应该是{}\".format(dW1.shape)\n",
    "    assert W2.shape == dW2.shape, \"W2的维度应该是{}\".format(dW2.shape)\n",
    "    assert b1.shape == db1.shape, \"b1的维度应该是{}\".format(db1.shape)\n",
    "    assert b2.shape == db2.shape, \"b2的维度应该是{}\".format(db2.shape)\n",
    "\n",
    "    # 以字典形式返回更新后的模型参数\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"W2\": W2,\n",
    "        \"b1\": b1,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T06:36:07.867382Z",
     "start_time": "2024-01-13T06:36:07.846779Z"
    }
   },
   "id": "2e15c9f683666890",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.8 搭建模型训练逻辑"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e778f517ca3e55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def nn_model(train_X, train_Y, iterarion_num=2000, learning_rate=0.005, TOL=1e-6, print_loss=False):\n",
    "    \"\"\"\n",
    "    模型训练\n",
    "    :param train_X: 训练集输入数据\n",
    "    :param train_Y: 训练集输出数据\n",
    "    :param iterarion_num: 迭代次数\n",
    "    :param learning_rate: 学习率\n",
    "    :param TOL: 两次损失函数差值阈值\n",
    "    :param print_loss: 是否打印损失函数值\n",
    "    :return: 模型参数\n",
    "    \"\"\"\n",
    "    # 获取输入层维度、隐藏层维度、输出层维度\n",
    "    n_x, n_h, n_y = layer_sizes(train_X, train_Y)\n",
    "\n",
    "    # 初始化模型参数\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    # 迭代训练\n",
    "    for i in trange(1, iterarion_num + 1):\n",
    "        # 前向传播\n",
    "        cache = forward_propogation(train_X, parameters)\n",
    "\n",
    "        # 计算损失函数\n",
    "        loss = loss_function(cache[\"A2\"], train_Y)\n",
    "\n",
    "        # 反向传播\n",
    "        grads = back_propogation(parameters, cache, train_X, train_Y)\n",
    "\n",
    "        # 更新模型参数\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # 打印损失函数值\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print(\"第{}次迭代，损失函数值为：{}\".format(i, loss))\n",
    "\n",
    "        # 判断损失函数是否小于阈值\n",
    "        if loss < TOL:\n",
    "            break\n",
    "\n",
    "    return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T06:48:11.782707Z",
     "start_time": "2024-01-13T06:48:11.758897Z"
    }
   },
   "id": "34b4ec252d86c130",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.9 模型预测函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab460cff15ed8244"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict(parameters, test_X):\n",
    "    \"\"\"\n",
    "    模型预测函数\n",
    "    :param parameters: 模型参数\n",
    "    :param test_X: 测试集输入数据\n",
    "    :return: 预测结果\n",
    "    \"\"\"\n",
    "    # 前向传播\n",
    "    cache = forward_propogation(test_X, parameters)\n",
    "\n",
    "    # 获取输出层激活值\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    # 将输出层激活值大于0.5的置为1，小于0.5的置为0\n",
    "    predictions = np.where(A2 > 0.5, 1, 0)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def calculate_accuracy(predictions, test_Y):\n",
    "    \"\"\"\n",
    "    计算准确率\n",
    "    :param predictions: 预测结果\n",
    "    :param test_Y: 测试集输出数据\n",
    "    :return: 准确率\n",
    "    \"\"\"\n",
    "    # 获取测试集样本数\n",
    "    m = test_Y.shape[1]\n",
    "\n",
    "    # 计算预测正确的样本数\n",
    "    correct_num = np.sum(predictions == test_Y)\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = correct_num / m\n",
    "    accuracy = np.squeeze(accuracy)\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T06:56:04.272110Z",
     "start_time": "2024-01-13T06:56:04.255824Z"
    }
   },
   "id": "22bf52266e3c87ff",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 模型训练与评估"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad137962d84960f5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 13626.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第100次迭代，损失函数值为：0.6920308003790124\n",
      "第200次迭代，损失函数值为：0.6908368971711158\n",
      "第300次迭代，损失函数值为：0.6887440581265976\n",
      "第400次迭代，损失函数值为：0.6848505160509772\n",
      "第500次迭代，损失函数值为：0.6776038298050123\n",
      "第600次迭代，损失函数值为：0.6644088910872272\n",
      "第700次迭代，损失函数值为：0.6412401921757865\n",
      "第800次迭代，损失函数值为：0.6031708750296307\n",
      "第900次迭代，损失函数值为：0.5468883280466772\n",
      "第1000次迭代，损失函数值为：0.4746543182744588\n",
      "第1100次迭代，损失函数值为：0.3955720560115348\n",
      "第1200次迭代，损失函数值为：0.3207991611307584\n",
      "第1300次迭代，损失函数值为：0.2575295153308824\n",
      "第1400次迭代，损失函数值为：0.20751913427520557\n",
      "第1500次迭代，损失函数值为：0.16923644957802975\n",
      "第1600次迭代，损失函数值为：0.14016398805132757\n",
      "第1700次迭代，损失函数值为：0.11795986375724161\n",
      "第1800次迭代，损失函数值为：0.10078974079650097\n",
      "第1900次迭代，损失函数值为：0.08731061417989991\n",
      "第2000次迭代，损失函数值为：0.07656268154212359\n",
      "准确率为：100.00%\n",
      "\n",
      "W1 = [[ 0.11342406  0.43252048 -0.66576254 -0.2641163 ]\n",
      " [-0.00943954 -0.03960593  0.07908106  0.017851  ]\n",
      " [-0.08257304 -0.31539016  0.479831    0.22740062]\n",
      " [-0.08025473 -0.33809228  0.50601636  0.21038116]\n",
      " [ 0.0194829   0.12926242 -0.20268628 -0.08302727]\n",
      " [ 0.04566165  0.20737098 -0.3128308  -0.14596489]\n",
      " [ 0.04587388  0.18896103 -0.2960177  -0.13678562]\n",
      " [-0.05394978 -0.19993945  0.31592646  0.1586371 ]]\n",
      "\n",
      "W2 = [[-1.04565562  0.08795996  0.7053272   0.74245762 -0.26477237 -0.43318006\n",
      "  -0.40335584  0.43565492]]\n",
      "\n",
      "b1 = [[ 0.07019131]\n",
      " [-0.00323191]\n",
      " [-0.0484312 ]\n",
      " [-0.05212443]\n",
      " [ 0.01770891]\n",
      " [ 0.03356209]\n",
      " [ 0.03066744]\n",
      " [-0.03060521]]\n",
      "\n",
      "b2 = [[-0.16065276]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "parameters = nn_model(train_X, train_Y, iterarion_num=2000, learning_rate=0.005, TOL=1e-6, print_loss=True)\n",
    "\n",
    "# 模型预测\n",
    "predictions = predict(parameters, test_X)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = calculate_accuracy(predictions, test_Y)\n",
    "\n",
    "print(\"准确率为：{:.2f}%\\n\".format(accuracy * 100))\n",
    "\n",
    "for param in parameters:\n",
    "    print(\"{} = {}\\n\".format(param, parameters[param]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T07:04:52.428307Z",
     "start_time": "2024-01-13T07:04:52.277520Z"
    }
   },
   "id": "86a0ce1098c507e5",
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
